import streamlit as st
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
import os
import re
from collections import Counter
import io
import altair as alt

# =============================
# ğŸ”§ ì´ˆê¸° ì„¤ì • & ìŠ¤íƒ€ì¼
# =============================
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

st.set_page_config(
    page_title="CS ì¼ì¼ë³´ê³  ìë™ ìš”ì•½ ìƒì„±ê¸°",
    page_icon="ğŸ“Š",
    layout="wide",
)

# ê¸€ë¡œë²Œ CSS
st.markdown("""
<style>
.app-header {font-size: 28px; font-weight:700; color:#2b6cb0; margin-bottom:4px}
.sub {color:#64748b; font-size:13px; margin-bottom:12px}
.card {border:1px solid #e2e8f0; border-radius:14px; padding:18px; background:#ffffff; box-shadow:0 2px 6px rgba(17,24,39,0.03);}
.kicker {font-weight:700; color:#0f172a; margin-bottom:10px}
.muted {color:#64748b; font-size:13px}
</style>
""", unsafe_allow_html=True)


# =============================
# ğŸ§° ìœ í‹¸ í•¨ìˆ˜ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# =============================
def normalize_col(name: str) -> str:
    if name is None:
        return ""
    s = str(name)
    s = re.sub(r"\s+", "", s)
    s = s.replace("\u00A0", "")
    return s

def build_column_map(cols):
    REQUIRED = {"êµ¬ë¶„", "ë‚´ìš©", "ì¹´í…Œê³ ë¦¬"}
    norm_to_actual = {normalize_col(c): c for c in cols}
    mapping = {}
    for need in REQUIRED:
        key = normalize_col(need)
        if key in norm_to_actual:
            mapping[norm_to_actual[key]] = need
    return mapping


CATEGORY_MAP = {  # âœ… ê·¸ëŒ€ë¡œ
    "ê°•ì¢Œ/ìƒí’ˆ ì‹ ì²­, ë°°ì†¡ - ê°•ì¢Œì‹ ì²­": "ë„¥ìŠ¤íŠ¸íŒ¨ìŠ¤/ê°•ì¢Œ/êµì¬ ì‹ ì²­, ë°°ì†¡",
    "ê°•ì¢Œ/ìƒí’ˆ ì‹ ì²­, ë°°ì†¡ - ìƒí’ˆì‹ ì²­": "ë„¥ìŠ¤íŠ¸íŒ¨ìŠ¤/ê°•ì¢Œ/êµì¬ ì‹ ì²­, ë°°ì†¡",
    "ê°•ì¢Œ/ìƒí’ˆ ì‹ ì²­, ë°°ì†¡ - ë°˜ì†¡": "ë„¥ìŠ¤íŠ¸íŒ¨ìŠ¤/ê°•ì¢Œ/êµì¬ ì‹ ì²­, ë°°ì†¡",
    "ê°•ì¢Œ/ìƒí’ˆ ì‹ ì²­, ë°°ì†¡ - ë°°ì†¡": "ë„¥ìŠ¤íŠ¸íŒ¨ìŠ¤/ê°•ì¢Œ/êµì¬ ì‹ ì²­, ë°°ì†¡",
    "ê°•ì¢Œ/ìƒí’ˆ ì‹ ì²­, ë°°ì†¡ - e-êµì¬ì‹ ì²­": "ë„¥ìŠ¤íŠ¸íŒ¨ìŠ¤/ê°•ì¢Œ/êµì¬ ì‹ ì²­, ë°°ì†¡",
    "ê²°ì œ, ì·¨ì†Œ, í™˜ë¶ˆ - ê²°ì œ": "ê²°ì œ/ì·¨ì†Œ/í™˜ë¶ˆ",
    "ê²°ì œ, ì·¨ì†Œ, í™˜ë¶ˆ - ì·¨ì†Œ/í™˜ë¶ˆ": "ê²°ì œ/ì·¨ì†Œ/í™˜ë¶ˆ",
    "ë™ì˜ìƒ ìˆ˜ê°•-PC - ë™ì˜ìƒ ì˜¤ë¥˜": "ë™ì˜ìƒ, ëª¨ë°”ì¼ ê¸°ê¸° ê´€ë ¨",
    "ë™ì˜ìƒ ìˆ˜ê°•-PC - PC ê¸°ê¸°": "ë™ì˜ìƒ, ëª¨ë°”ì¼ ê¸°ê¸° ê´€ë ¨",
    "ëª¨ë°”ì¼ ê¸°ê¸° - ëª¨ë°”ì¼ ê¸°ê¸°": "ë™ì˜ìƒ, ëª¨ë°”ì¼ ê¸°ê¸° ê´€ë ¨",
    "ì‚¬ì´íŠ¸ ì´ìš© - ë¶€ì •ì‚¬ìš©": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ì‚¬ì´íŠ¸ ì´ìš© - ì‚¬ì´íŠ¸ ì˜¤ë¥˜": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ì‚¬ì´íŠ¸ ì´ìš© - ìˆ˜ê°•ê¸°ê°„": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ì‚¬ì´íŠ¸ ì´ìš© - ì—…ë¡œë“œ": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ì‚¬ì´íŠ¸ ì´ìš© - ì´ë²¤íŠ¸": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ì‚¬ì´íŠ¸ ì´ìš© - í•™ë ¥ì˜ˆì¸¡ í’€ì„œë¹„ìŠ¤": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ì‚¬ì´íŠ¸ ì´ìš© - íŒ¨ìŠ¤ í™˜ê¸‰": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ì‚¬ì´íŠ¸ ì´ìš© - íŒ¨ìŠ¤ ì—°ì¥": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "íšŒì›ì •ë³´ - ì¼ë°˜íšŒì›": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ê³µë¬´ì› ìˆ˜í—˜ì •ë³´ - ê³µë¬´ì› ìˆ˜í—˜ì •ë³´": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ê¸°íƒ€ ë¬¸ì˜ - ê±´ì˜ì‚¬í•­": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ê¸°íƒ€ ë¬¸ì˜ - ë„¥ìŠ¤íŠ¸ì„ ìƒë‹˜": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ê¸°íƒ€ ë¬¸ì˜ - ë„¥ìŠ¤íŠ¸ìŠ¤í„°ë”” í•™ì›": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ê¸°íƒ€ ë¬¸ì˜ - ê¸°íƒ€": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
}
def map_category(v): return CATEGORY_MAP.get(str(v).strip(), v)

# âœ… ê°•ì‚¬ ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ
TEACHER_NAMES = [    "ì„ì§€í˜œ", "ì´ìœ¤ì£¼", "ì¡°íƒœì •", "ì„±ì •í˜œ", "ê²½ì„ ì‹", "ë°•ë…¸ì¤€",
    "ë°•ìˆ˜ì—°", "ê³ ì¢…í›ˆ", "ë¼ì˜í™˜", "ìµœì˜ì¬", "ì „í•œê¸¸",
    "ë°•ì°¬í˜", "í™©ì² ê³¤", "ì´ìƒí—Œ", "ì‹ ìš©í•œ", "ì „íš¨ì§„", "ì •ì¸êµ­", "ì–‘ìŠ¹ìš°",
    "ì´ìƒí˜„", "ì¡°ì—¬ì€", "ì„œí˜¸ì„±", "ì¥ë³‘ì—´", "ì‹ ëª…", "ë°•ìƒë¯¼", "ê³ ë¹„í™˜",
    "ê¹€ê´‘í›ˆ", "ê¹€í˜•ì¤€", "ì˜¤ì •í™”", "ë‚¨ì •ì„ ", "ë°±ê´‘í›ˆ", "í—ˆì„œìœ ", "ì˜¤ì œí˜„",
    "ì´ì¢…í•˜", "ìµœí¬ì¤€", "ê¹€ì°½í›ˆ", "ì´ì§„ì˜¤", "ì§„ìŠ¹í˜„", "ì†¡ì•„ë¦„", "ì´ì¬í›ˆ",
    "ì†¡ì•„ì˜", "ê¹€ì¢…í™˜", "ì‹¬ìŠ¹ì•„", "ê³½ë™ì§„", "ì •ì¸êµ­", "ì„ì¬í¬"
]
def detect_teacher(text):
    t = str(text)
    for n in TEACHER_NAMES:
        if n in t:
            return n
    return None

def preprocess_text(texts):
    safe = [str(t) for t in texts if pd.notna(t)]
    words = re.findall(r"[ê°€-í£A-Za-z]+", " ".join(safe))
    return [w for w in words if len(w) > 1]


# âœ… ì„¸ì…˜ ìƒíƒœ
for key, default in {
    "analyzed": False,
    "keyword_buffer": None,
    "keyword_count": 0,
    "cards_payload": None,
    "report_text": None
}.items():
    st.session_state.setdefault(key, default)


st.markdown('<div class="app-header">ğŸ“Š CS ì¼ì¼ë³´ê³  ìë™ ìš”ì•½ ìƒì„±ê¸°</div>', unsafe_allow_html=True)

client = OpenAI(api_key=api_key)

# âœ… ì‚¬ì´ë“œë°” UI ì•ˆë‚´ ë³µêµ¬
st.sidebar.header("ë„ì›€ë§ â”")
st.sidebar.markdown(
    """
    ì¼ì¼ì—…ë¬´ ë³´ê³  ì¤‘ ì¹´í…Œê³ ë¦¬ ë³„ 
    ì ‘ìˆ˜ ë‚´ìš©ì„ ìš”ì•½ì •ë¦¬í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.

    âœ… FAQ ê´€ë¦¬ì ì†¡ìˆ˜ì‹ ê´€ë¦¬ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ

    âœ… ì—‘ì…€ ì €ì¥ ë°©ì‹ì„ .xslx ë¡œ ë³€ê²½ 
    
    âœ… íŒŒì¼ ì—…ë¡œë“œ

    """
)

# ============================================================
# âœ… íƒ­ UI êµ¬ì„± (ìš”ì•½ / ê±´ìˆ˜ í†µê³„)
# ============================================================
tab1, tab2 = st.tabs(["ğŸ” ìš”ì•½ ìƒì„±", "ğŸ“Š ë¬¸ì˜ ê±´ìˆ˜ í†µê³„"])

# âœ… TAB 1 : ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€
with tab1:
    uploaded_file = st.file_uploader("ğŸ“‚ ì—‘ì…€ ì—…ë¡œë“œ (.xlsx)")
    run = st.button("ğŸ” ìš”ì•½ ìƒì„±í•˜ê¸°")

    # ğŸ” ê¸°ì¡´ ìš”ì•½ ìƒì„± ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€
    if run and uploaded_file:
        st.session_state["analyzed"] = False
        status_box = st.empty()
        status_box.info("ğŸ”„ ë¬¸ì˜ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")

        df = pd.read_excel(uploaded_file)
        df = df.rename(columns=build_column_map(df.columns))
        df["ëŒ€í‘œì¹´í…Œê³ ë¦¬"] = df["ì¹´í…Œê³ ë¦¬"].apply(map_category)
        df["ê°•ì‚¬ëª…"] = df["ë‚´ìš©"].apply(detect_teacher)

        q_df = df[df["êµ¬ë¶„"] == "Q"]
        grouped = q_df.groupby("ëŒ€í‘œì¹´í…Œê³ ë¦¬")["ë‚´ìš©"].apply(list).to_dict()

        KEYWORDS = ["ì¤‘ë³µ", "iOS", "í”Œë ˆì´ì–´ ID", "ì¶©ëŒì´ìŠˆ", "ì´ˆê¸°í™”"]
        keyword_df = df[
            (df["ëŒ€í‘œì¹´í…Œê³ ë¦¬"] == "ë™ì˜ìƒ, ëª¨ë°”ì¼ ê¸°ê¸° ê´€ë ¨") & 
            (df["ë‚´ìš©"].str.contains("|".join(KEYWORDS), case=False, na=False))
        ]

        st.session_state["keyword_count"] = len(keyword_df)
        buf = io.BytesIO()
        keyword_df.to_excel(buf, index=False)
        buf.seek(0)
        st.session_state["keyword_buffer"] = buf

        cards_payload = []
        all_lines = []
        progress = st.progress(0)
        cols = st.columns(2)

        for i, (cat, qs) in enumerate(grouped.items(), start=1):
            common = [w for w, _ in Counter(preprocess_text(qs)).most_common(10)]
            teachers = q_df[q_df["ëŒ€í‘œì¹´í…Œê³ ë¦¬"] == cat]["ê°•ì‚¬ëª…"].dropna().unique()
            mention = (
                f"íŠ¹ì • ê°•ì‚¬ ê´€ë ¨ ë¬¸ì˜ í¬í•¨: {', '.join(teachers)} ì„ ìƒë‹˜ ê´€ë ¨ ë¬¸ì˜ í¬í•¨."
                if len(teachers) else ""
            )

            joined = "\n".join([str(x) for x in qs[:30]])

            prompt = f"""
            ì•„ë˜ëŠ” '{cat}' ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” íšŒì› ë¬¸ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.
            ì´ {len(qs)}ê±´ì˜ ë¬¸ì˜ê°€ ìˆìŠµë‹ˆë‹¤.
            ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(common)}
            {mention}
            ì•„ë˜ ì¡°ê±´ì„ ë°˜ë“œì‹œ ì§€ì¼œ ìš”ì•½í•˜ì„¸ìš”.
            1. ê° ì¤„ì€ ë°˜ë“œì‹œ ì–´ë–¤ ë¬¸ì˜ì¸ì§€ ì•Œ ìˆ˜ ìˆë„ë¡ ê°„ëµí•˜ê²Œ ì •ë¦¬í•´ì„œ '~ ê´€ë ¨ ë¬¸ì˜ ì ‘ìˆ˜' í˜•íƒœë¡œ ëë‚  ê²ƒ.
            2. ê°•ì‚¬ëª…ì´ í¬í•¨ëœ ê²½ìš° ë°˜ë“œì‹œ ëª…ì‹œí•  ê²ƒ (ì˜ˆ: 'ìœ íœ˜ìš´ ì„ ìƒë‹˜ êµì¬ ê´€ë ¨ ë¬¸ì˜ ì ‘ìˆ˜').
            3. ë¶ˆí•„ìš”í•œ ì„¤ëª…, ì›ì¸, ì‚¬ìœ , ë¬¸ì¥í˜• í•´ì„¤ ê¸ˆì§€.
            4. ìµœëŒ€ 5ì¤„ê¹Œì§€ë§Œ ì‘ì„±.
            5. ê°™ì€ ì˜ë¯¸ì˜ ë¬¸ì˜ëŠ” í•˜ë‚˜ë¡œ ë¬¶ì„ ê²ƒ.

            ë¬¸ì˜ ë‚´ìš©:
            {joined}
            """

            try:
                resp = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}]
                )
                summary = resp.choices[0].message.content.strip()
            except:
                summary = "(ìš”ì•½ ì‹¤íŒ¨)"

            cards_payload.append({"cat": cat, "count": len(qs), "summary": summary})
            all_lines.append(f"**[{cat}]**\n{summary}\n")
            progress.progress(i / len(grouped))

        status_box.empty()

        st.session_state["cards_payload"] = cards_payload
        st.session_state["report_text"] = "\n".join(all_lines)
        st.session_state["analyzed"] = True

    # âœ… ìš”ì•½ ê²°ê³¼ ì¶œë ¥
    if st.session_state["analyzed"]:
        st.success("âœ… ë¶„ì„ ì™„ë£Œ!")

        st.markdown(
            f"<div class='card'><strong>- iOS ê¸°ê¸°ID ê´€ë ¨ ê¸°ê¸°ì‚­ì œ/í•´ê²° ìš”ì²­ ë¬¸ì˜ ì•½ {st.session_state['keyword_count']}ê±´</strong></div>",
            unsafe_allow_html=True,
        )

        st.download_button(
            "ğŸ“¥ í‚¤ì›Œë“œ ì¶”ì¶œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ",
            st.session_state["keyword_buffer"],
            "keyword_extracted_data.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        cols = st.columns(2)
        for i, item in enumerate(st.session_state["cards_payload"], start=1):
            with cols[i % 2]:
                st.markdown(f"""
                    <div class='card'>
                        <div class='kicker'>ğŸ“‚ [{item['cat']}]</div>
                        <div class='muted'>ì´ {item['count']}ê±´</div>
                        <div style='white-space:pre-wrap;'>{item['summary']}</div>
                    </div>
                """, unsafe_allow_html=True)

        st.download_button(
            "ğŸ“¥ ì „ì²´ ìš”ì•½ ë‹¤ìš´ë¡œë“œ",
            st.session_state["report_text"].encode("utf-8"),
            "CS_ì¼ì¼ë³´ê³ _v4_ê°•ì‚¬í¬í•¨.txt",
            "text/plain",
        )


CATEGORY_MAP_MAIN = {
    "ê°•ì¢Œ/ìƒí’ˆ ì‹ ì²­, ë°°ì†¡": "ë„¥ìŠ¤íŠ¸íŒ¨ìŠ¤/ê°•ì¢Œ/êµì¬ ì‹ ì²­, ë°°ì†¡",
    "ê²°ì œ, ì·¨ì†Œ, í™˜ë¶ˆ": "ê²°ì œ/ì·¨ì†Œ/í™˜ë¶ˆ",
    "ë™ì˜ìƒ ìˆ˜ê°•-PC": "ë™ì˜ìƒ, ëª¨ë°”ì¼ ê¸°ê¸° ê´€ë ¨",
    "ëª¨ë°”ì¼ ê¸°ê¸°": "ë™ì˜ìƒ, ëª¨ë°”ì¼ ê¸°ê¸° ê´€ë ¨",
    "ì‚¬ì´íŠ¸ ì´ìš©": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "íšŒì›ì •ë³´": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ê³µë¬´ì› ìˆ˜í—˜ì •ë³´": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
    "ê¸°íƒ€ ë¬¸ì˜": "í™ˆí˜ì´ì§€/ì´ë²¤íŠ¸ ê´€ë ¨",
}

def map_main_category(v):
    v = str(v).strip()
    return CATEGORY_MAP_MAIN.get(v, v)  # ë§¤í•‘ ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€

with tab2:
    st.header("ğŸ“Š ëŒ€ë¶„ë¥˜ ê¸°ë°˜ ë¬¸ì˜ ê±´ìˆ˜ í†µê³„")

    uploaded_file_2 = st.file_uploader("ğŸ“‚ í†µê³„ìš© ì—‘ì…€ ì—…ë¡œë“œ (.xlsx)", key="stats_file")

    if uploaded_file_2:
        df2 = pd.read_excel(uploaded_file_2)
        df2 = df2.rename(columns=lambda x: str(x).strip())

        if "ëŒ€ë¶„ë¥˜" not in df2.columns or "ë¬¸ì˜ëŸ‰" not in df2.columns:
            st.error("âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: 'ëŒ€ë¶„ë¥˜', 'ë¬¸ì˜ëŸ‰' í•„ìš”")
            st.dataframe(df2.head())
            st.stop()

        df2["ëŒ€í‘œì¹´í…Œê³ ë¦¬"] = df2["ëŒ€ë¶„ë¥˜"].apply(map_main_category)

        count_df = df2.groupby("ëŒ€í‘œì¹´í…Œê³ ë¦¬")["ë¬¸ì˜ëŸ‰"].sum().reset_index()
        count_df = count_df.sort_values(by="ë¬¸ì˜ëŸ‰", ascending=False)

        st.subheader("ğŸ“Œ í†µê³„ ê²°ê³¼")
        for _, row in count_df.iterrows():
            st.write(f"[{row['ëŒ€í‘œì¹´í…Œê³ ë¦¬']}] : {int(row['ë¬¸ì˜ëŸ‰'])}ê±´")

        st.divider()
        st.dataframe(count_df, use_container_width=True)

        chart = alt.Chart(count_df).mark_bar().encode(
            x=alt.X("ëŒ€í‘œì¹´í…Œê³ ë¦¬:N", sort="-y", title="ì¹´í…Œê³ ë¦¬"),
            y=alt.Y("ë¬¸ì˜ëŸ‰:Q", title="ë¬¸ì˜ìˆ˜"),
            tooltip=["ëŒ€í‘œì¹´í…Œê³ ë¦¬", "ë¬¸ì˜ëŸ‰"]
        ).properties(height=400)

        st.altair_chart(chart, use_container_width=True)

    else:
        st.info("ğŸ“‚ í†µê³„ìš© ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
